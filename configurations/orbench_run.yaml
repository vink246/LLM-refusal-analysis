dataset_dir: "/home/hice1/ccheah6/scratch/NLP/LLM-refusal-analysis/datasets/or-bench"         # Path to processed OR-Bench dataset
categories:                                 # OR-Bench categories we want to analyze refusal across (all 10)
  - deception
  - harassment  
  - harmful
  - hate
  - illegal
  - privacy
  - self-harm
  - sexual
  - unethical
  - violence
num_samples_per_category: 1000         
shuffle: true                               # shuffle order of data 
models:                                     # path to inferring models
  - "meta-llama/Llama-2-7b-chat-hf"
  - "mistralai/Mistral-7B-Instruct-v0.1"
result_dir: "results"
save_activations: true
activation_layers:
  - "residuals_10"
  - "residuals_15"
  - "mlp_11"
batch_size: 4
device: "cuda"
# SAE Training Configuration
# Note: SAE training balancing is independent of inference-time data loading
sae_training:
  balance_strategy: "stratified"  # "none", "stratified", or "weighted"
  samples_per_category: 100  # Target samples per category after balancing (None = use all available)
  safe_toxic_ratio: 0.5  # Target ratio of safe to toxic (0.5 = equal split)
  max_samples_per_category: 1000  # Maximum samples per category for downsampling

# Circuit Discovery Configuration
discover_separate_safe_toxic: true  # If true, discover separate circuits for safe and toxic within each category

analysis:
  similarity_threshold: 0.8
  activation_ratio_threshold: 2.0
  modularity_assessment:
    monolithic_threshold: 0.8
    partially_modular_threshold: 0.5